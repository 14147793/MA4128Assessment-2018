Data Engineering
==================
Darren Kelly 14163535


Data engineering is a practice across of a number of disciplines which includes engineering computing systems, computer software, or taking information through the analysis of data. This following essay will briefly go through what data engineers do.

Data engineers are the data proffessionals who prepare the "big data" infrastructure and build massive reservoirs for big data. They develop, construct, test and maintain architectures such as databases and large-scale data processing systems. Once continuous pipelines are installed to – and from – these huge “pools” of filtered information, data scientists can pull relevant data sets for their analysis. A data engineer allow data scientists to do their jobs more effectively. A data engineer gathers and collects the data, stores it, does batch processing or real-time processing on it, and serves it via an API (Application Program Interface) to a data scientist who can easily query it.

There are a lot of Big Data tools that perform each of these steps, and it is important that they can back up their choice of using a particular tool. A good data engineer has huge knowledge of databases and use best engineering practices. These include handling and logging erros, monitoring the system, building human-fault-tolerant pipelines, understanding what is necessary to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning, and ensuring a deterministic pipeline. Then, they write complex queries on that, make sure it is easily optimizing the performance of their company's big data ecosystem. They might also run some ETL (Extract, Transform and Load) on top of big datasets and create big data warehouses that can be used for reporting or analysis by data scientists. Beyond that, because data engineers focus more on the design and architecture, they are typically not expected to know any machine learning or analytics for big data.


